{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "associate-hybrid",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd M:/Rafael/Documentos/GitHub/TCC\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from preprocessa import recupera_arquivos_data\n",
    "from vector import recupera_arquivos_token\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "favorite-alliance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importa bibliotecas e carrega modelo Word2Vec\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "wv = KeyedVectors.load(\"artigos.wv\", mmap='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prepared-federal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo de word-vector \n",
    "print(*list(wv['política']), sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decimal-contest",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plota_similaridade(palavra, n):\n",
    "    ''' Representa através de gráfico de barras o grau de similaridade de uma palavra com as N mais próximas '''\n",
    "    lista = wv.most_similar(positive=[palavra], negative=[], topn=n)\n",
    "    df = pd.DataFrame(lista, columns=['palavra', 'grau similaridade'])\n",
    "    df = df.set_index('palavra')\n",
    "    df['grau similaridade'] = (df['grau similaridade']*100).astype(int)\n",
    "    \n",
    "    df.plot(kind=\"barh\",title=palavra,  legend=False, width=0.95)\n",
    "    plt.axis(\"off\")\n",
    "    for i, (p, pr) in enumerate(zip(df.index, df[\"grau similaridade\"])):\n",
    "        plt.text(s=p, x=1, y=i, color=\"w\", verticalalignment=\"center\", size=14)\n",
    "        plt.text(s=str(pr)+\"%\", x=pr-12, y=i, color=\"w\",\n",
    "                 verticalalignment=\"center\", horizontalalignment=\"left\", size=12)\n",
    "    plt.rcParams[\"figure.figsize\"] = (4,3)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.savefig(\"sim_\" + palavra +\".png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grand-citizenship",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plota_similaridade(palavra, n):\n",
    "    ''' Representa através de gráfico de barras o grau de similaridade de uma palavra com as N mais próximas '''\n",
    "    lista = wv.most_similar(positive=[palavra], negative=[], topn=n)\n",
    "    df = pd.DataFrame(lista, columns=['palavra', 'grau similaridade'])\n",
    "    df = df.set_index('palavra')\n",
    "    df['grau similaridade'] = (df['grau similaridade']*100).astype(int)\n",
    "    \n",
    "    df.plot(kind=\"barh\",title=palavra,  legend=False, width=0.95)\n",
    "    plt.axis(\"off\")\n",
    "    for i, (p, pr) in enumerate(zip(df.index, df[\"grau similaridade\"])):\n",
    "        plt.text(s=p, x=1, y=i, color=\"w\", verticalalignment=\"center\", size=14)\n",
    "        plt.text(s=str(pr)+\"%\", x=pr-12, y=i, color=\"w\",\n",
    "                 verticalalignment=\"center\", horizontalalignment=\"left\", size=12)\n",
    "    plt.rcParams[\"figure.figsize\"] = (4,3)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.savefig(\"sim_\" + palavra +\".png\")\n",
    "    \n",
    "for palavra in ['lula', 'dilma', 'dória', 'marina', 'ciro', 'bolsonaro', 'eleição', 'trump',\n",
    "                'esquerda', 'direita', 'cloroquina', 'covid', 'pandemia', 'china', 'vacina']:\n",
    "    plota_similaridade(palavra, 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blond-values",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carrega o dataset\n",
    "df = pd.read_csv('dataset_final.csv', encoding='utf-8',sep=';', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "revolutionary-spare",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "virtual-wages",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(df['portal']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floppy-auditor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando DataFrame com documentos de 'Esquerda' e removendo as colunas 'portal' e 'posição'\n",
    "df_e = df.loc[df['posicao'] == 'Esquerda']\n",
    "df_e = df_e.drop(['portal', 'posicao'], axis=1)\n",
    "df_e               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impressive-ordinary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando DataFrame com documentos de 'Direita' e removendo as colunas 'portal' e 'posição'\n",
    "df_d = df.loc[df['posicao'] == 'Direita']\n",
    "df_d = df_d.drop(['portal', 'posicao'], axis=1)\n",
    "df_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tough-activation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertendo DataFrames para listas de artigos, contendo cada um uma lista de palavras\n",
    "dados_d = df_d.values.tolist()\n",
    "dados_e = df_e.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "critical-melbourne",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dados_e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "signal-joseph",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dados_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funky-investor",
   "metadata": {},
   "outputs": [],
   "source": [
    "def monta_palavras_vec(dados):\n",
    "    ''' Retorna um NumPy Arrray de documentos X features. Cada documento possui 300 dimensões, ou features p/ ML '''\n",
    "    artigos_vec = np.zeros((len(dados), 300))\n",
    "    tamanho = len(dados)\n",
    "    print(artigos_vec.shape)\n",
    "    # Monta vetores para todos os documentos do dataset\n",
    "    for i, artigo in tqdm(enumerate(dados), total=tamanho, desc='Processando', delay=1):\n",
    "        for palavra in artigo:\n",
    "            # Interrompe ao encontrar NaN (fim do artigo)\n",
    "            if type(palavra) != str:\n",
    "                break\n",
    "            artigos_vec[i] += wv[palavra] \n",
    "        artigos_vec[i] /= len(artigo)   \n",
    "    return artigos_vec\n",
    "    \n",
    "artigos_vec_d = monta_palavras_vec(dados_d)        \n",
    "artigos_vec_e = monta_palavras_vec(dados_e)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effective-synthesis",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "# Downsampling da classe majoritária (Esquerda)\n",
    "downsampled = resample(artigos_vec_e, \n",
    "                       replace=True,     \n",
    "                       n_samples=artigos_vec_d.shape[0],    \n",
    "                       random_state=123) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informative-utility",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenando coluna com zeros para rotular a classe 'E' ou 'Esquerda' \n",
    "artigos_vec_e = downsampled\n",
    "classe_e = np.zeros((artigos_vec_e.shape[0], 1), dtype=np.int8)\n",
    "artigos_vec_e = np.concatenate((artigos_vec_e, classe_e), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "streaming-school",
   "metadata": {},
   "outputs": [],
   "source": [
    "artigos_vec_e.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "restricted-episode",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenando coluna com digito 1 para rotular a classe 'D' ou 'Direita'\n",
    "classe_d = np.ones((artigos_vec_d.shape[0], 1), dtype=np.int8)\n",
    "artigos_vec_d = np.concatenate((artigos_vec_d, classe_d), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ongoing-courtesy",
   "metadata": {},
   "outputs": [],
   "source": [
    "artigos_vec_d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "random-budget",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatena as duas classes em uma só matriz\n",
    "artigos_vec = np.concatenate((artigos_vec_d, artigos_vec_e))\n",
    "artigos_vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "honest-banking",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrando que não existem 'missing values' no dataset\n",
    "print(np.where(np.isnan(artigos_vec)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loose-general",
   "metadata": {},
   "outputs": [],
   "source": [
    "artigos_vec.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "obvious-elizabeth",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(artigos_vec)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "innocent-joining",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importa bibliotecas do scikit-learn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "# Train/Test Split\n",
    "tam_teste = 0.25\n",
    "X = artigos_vec[:,:300]\n",
    "y = artigos_vec[:,-1:].ravel()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=tam_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "color-soldier",
   "metadata": {},
   "outputs": [],
   "source": [
    "# library\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# create data\n",
    "tamanho_grupos=[round((1-tam_teste)*100), round(tam_teste*100)]\n",
    "\n",
    "label_treino = f'     Dados Teste ({tamanho_grupos[1]}%)\\n\\n     {y_test.shape[0]:_} registros     '.replace(\"_\", \".\")\n",
    "label_teste = f'Dados Treino ({tamanho_grupos[0]}%) \\n\\n {y_train.shape[0]:_} registros      '.replace(\"_\", \".\")\n",
    "labels = label_teste, label_treino        \n",
    "\n",
    "# Create a pieplot\n",
    "plt.pie(tamanho_grupos, explode=(0, 0.06), labels=labels, autopct='',\n",
    "        shadow=True, startangle=0)\n",
    "\n",
    "# add a circle at the center to transform it in a donut chart\n",
    "my_circle=plt.Circle( (0,0), 0.6, color='white')\n",
    "p=plt.gcf()\n",
    "p.gca().add_artist(my_circle)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alpine-riding",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relatorio_classificacao(nome_modelo, resultado_modelo, y_test):\n",
    "    print(f'\\nAlgoritmo - {nome_modelo}\\n\\nRelatório de Classificação\\n\\n', \n",
    "          classification_report(y_test, resultado_modelo),'\\n\\nMatriz de Confusão\\n\\n', \n",
    "          confusion_matrix(resultado_modelo, y_test))   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "delayed-martin",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando modelo e treinando com os dados de treino\n",
    "clr = LogisticRegression(max_iter=300)\n",
    "clr.fit(X_train, y_train)\n",
    "# Fazendo a predição nos dados de teste\n",
    "resultado_clr = clr.predict(X_test)\n",
    "# Principais métricas de performance e Matriz de Confusão\n",
    "relatorio_classificacao(\"Regressão Logística\", resultado_clr, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confidential-mount",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando modelo e treinando com os dados de treino\n",
    "dtc = DecisionTreeClassifier()\n",
    "dtc.fit(X_train, y_train)\n",
    "# Fazendo a predição nos dados de teste\n",
    "resultado_dtc = dtc.predict(X_test)\n",
    "# Principais métricas de performance e Matriz de Confusão\n",
    "relatorio_classificacao(\"Árvore de Decisão\", resultado_dtc, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tutorial-thesaurus",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando modelo e treinando com os dados de treino\n",
    "rf=RandomForestClassifier(n_jobs=6, n_estimators=400)\n",
    "rf.fit(X_train,y_train)\n",
    "# Fazendo a predição nos dados de teste\n",
    "resultado_rf = rf.predict(X_test)\n",
    "# Principais métricas de performance e Matriz de Confusão\n",
    "relatorio_classificacao(\"Random Forest\", resultado_rf, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "completed-charm",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando modelo e treinando com os dados de treino\n",
    "ada = AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
    "                   n_estimators=50, random_state=123)\n",
    "ada.fit(X_train, y_train)\n",
    "# Fazendo a predição nos dados de teste\n",
    "resultado_ada = ada.predict(X_test)\n",
    "# Principais métricas de performance e Matriz de Confusão\n",
    "relatorio_classificacao(\"AdaBoost\", resultado_ada, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "casual-musician",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando modelo e treinando com os dados de treino\n",
    "knn = KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
    "                           metric_params=None, n_jobs=6, n_neighbors=5, p=2,\n",
    "                           weights='uniform')\n",
    "knn.fit(X_train, y_train)                           \n",
    "# Fazendo a predição nos dados de teste\n",
    "resultado_knn = knn.predict(X_test)\n",
    "# Principais métricas de performance e Matriz de Confusão\n",
    "relatorio_classificacao(\"K-Nearest Neighbors\", resultado_knn, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
